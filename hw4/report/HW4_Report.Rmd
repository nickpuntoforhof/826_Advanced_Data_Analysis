---
title: "BMI 826 - Homework 4"
author: "Noah Stafford"
output:
  html_document:
    df_print: paged
---

```{r knitr_options , include=FALSE}
knitr::opts_chunk$set(fig.width=12, fig.height=4,
fig.path='Figs/', warning=FALSE,
message=FALSE)
set.seed(53079239)
```


```{r, include=FALSE}
# libraries
library(tidyverse)
library(readxl)
library(e1071)
library(rpart)
library(ROCR)
library(BBmisc)
library(rminer)
library(caret)
library(fastDummies)
```

# Data Wrangling

Let's look at what the data looks like right after its imported.
```{r, echo=FALSE}
# Clean data and save to .RData file
nh_diab <- read.csv('../rawData/nhanes_diabetes.csv', stringsAsFactors = FALSE)
summary(nh_diab)
```
From the summary output, it's clear that this is pretty clean data. As a rule of thumb, when a column is imported into an R dataframe and is automatically coerced to an int/numeric with correclty coded NA's, that eliminates the possiblity of a wide array of potential problems with that column.  For instance, there can be no text, NAs coded a different way than expected, or certain types of typos.  There can still be errors/outlier within the values of a column, but the output above is telling me that most of the data cleaning work has been done already.

Now let's look at the data to get a sense of what we have:
```{r, echo = FALSE}
nh_diab_plot <- nh_diab %>% gather(key = "covariate", value = "value", -id)
covariate_names <- colnames(nh_diab)

ggplot(nh_diab_plot %>% filter(covariate %in% covariate_names[2:10]), aes(x=value)) + geom_histogram() + facet_wrap(~covariate, scales = "free", ncol=3) + theme_light()

ggplot(nh_diab_plot %>% filter(covariate %in% covariate_names[11:length(covariate_names)]), aes(x=value)) + geom_histogram() + facet_wrap(~covariate, scales = "free", ncol=3) + theme_light()
```
Some notes: alcohol appears to have a lot of zeros with sparse values higher than about 20 being sparse.  In all, this seems to be very good data.  There are clear normal distributions were they are expected, and the dataset appears to have been gathered from a population that is diverse demographically, with a large amount of observations for each category of each covariate.

There appears to be an odd encoding for age_smoking, where 0 designates "never smoked cigarettes regularly". Since this is a population of people with non-smokers, this covariate is really an interaction term between the age started smoking and smoker/non-smoker.  Deriving a smoker/non-smoker variable from this covariate will probably be informative.

household_income also displays an odd deviation from normality, which a large spike in density at value 11, which is encoded as income >= 75k, which was clearly too low of a threshold to abandon the windowing system that was used to encode the previous 10 variables, as a large portion of the distribution has been compressed into this category.  Furthermore, value 12 and value 13 represent patients where less information is known about there income, giving a simple > 20k or < 20k income for these patients.  

The issue, of course, is that perhaps the least balanced variable in the dataset is the one we are trying to predict.  The datset has 1,000 instances of people who have "heavy work" for their activity, but how many of those people are diabetic?
```{r, echo = FALSE}
nh_diab_plot_diab <- nh_diab %>% gather(key = "covariate", value = "value", -c(id,diabetic))
covariate_names <- unique(nh_diab_plot_diab$covariate)

ggplot(nh_diab_plot_diab %>% filter(covariate %in% covariate_names[1:3]), aes(x=value)) + geom_histogram() + facet_wrap(covariate~diabetic, scales = "free", ncol=2) + theme_light()

ggplot(nh_diab_plot_diab %>% filter(covariate %in% covariate_names[4:6]), aes(x=value)) + geom_histogram() + facet_wrap(covariate~diabetic, scales = "free", ncol=2) + theme_light()

ggplot(nh_diab_plot_diab %>% filter(covariate %in% covariate_names[7:9]), aes(x=value)) + geom_histogram() + facet_wrap(covariate~diabetic, scales = "free", ncol=2) + theme_light()

ggplot(nh_diab_plot_diab %>% filter(covariate %in% covariate_names[10:12]), aes(x=value)) + geom_histogram() + facet_wrap(covariate~diabetic, scales = "free", ncol=2) + theme_light()

ggplot(nh_diab_plot_diab %>% filter(covariate %in% covariate_names[13:15]), aes(x=value)) + geom_histogram() + facet_wrap(covariate~diabetic, scales = "free", ncol=2) + theme_light()

ggplot(nh_diab_plot_diab %>% filter(covariate %in% covariate_names[16]), aes(x=value)) + geom_histogram() + facet_wrap(covariate~diabetic, scales = "free", ncol=2) + theme_light()
```

By splitting the histograms on the response variable, we're looking for covariates with few observations within one category.  It can  be seen that we're getting only about 20 observations in some of the income bins for the diabetic patients.  The income bins needs to be widened, and the 12th and 13th bins need be incorporated into the scale of the others to create a strictly ordinal variable.  I binned the variables into low, middle, and high levels, coded simply as -1, 0, 1.

```{r, insert=FALSE}
nh_diab$income_bins <- ifelse(nh_diab$household_income %in% c(1,2,3,4,13), -1,
                                   ifelse(nh_diab$household_income %in% c(5,6,7,8,12), 0, ifelse(is.na(nh_diab$household_income), 0, 1)))
nh_diab <- nh_diab %>% select(-household_income)
```

# Missing Data Imputation

There is a significant amount of missing data.
```{r, echo=FALSE}
nh_diab_na <- nh_diab[complete.cases(nh_diab), ]
```

There are `r nrow(nh_diab)` rows in the data.  Removing all obervations with at least one missing missing observation, the dataset shrinks to `r nrow(nh_diab_na)`.  I would rather work with all the information present.  Let's look at which columns have missing data, and how much:

```{r, echo=FALSE}
apply(nh_diab, 2, function(x) sum(is.na(x)))
```

From this table we can see that the age_smoking variable is by far the largest issue in this regard. We have no missing data for the 'diabetic' response variable, which is good.  Since this dataset is small enough, and since this is survey population health data, I don't think that using some sort of advanced data imputation would be very useful.  For the numeric variables, the population mean will be substituted.  For ordinal categorial variables such as activity and education, I took the median instead of the mean to preserve the categorical nature of the data.  The boolean variables were stochastically assigned 0 and 1 with probability p equal to the populaton mean.  Finally, age_smoking proved the largest challenge.  I hypothesized that, since so much data was missing, that perhaps there is some information there.  So what's been done is two variables have been created -- smoking_na is a binary variable where 1 = NA obervation and 0 = non-NA.  For smoking_yes, 1 = smoking_age > 0 and not na.  The NAs in the age_smoking column were all coerced to 0's.

```{r, include=FALSE}
fit_data <- nh_diab

numeric_column_indexes <- c(2, 3, 4, 5, 6, 7, 10)
for(i in numeric_column_indexes){
  fit_data[is.na(fit_data[,i]), i] <- mean(fit_data[,i], na.rm = TRUE)
}

ordinal_column_indexes <- c(9, 12)
for(i in ordinal_column_indexes){
  fit_data[is.na(fit_data[,i]), i] <- median(fit_data[,i], na.rm = TRUE)
}

boolean_column_indexes <- c(8, 11)
for(i in boolean_column_indexes){
  fit_data[is.na(fit_data[,i]), i] <- rbernoulli(1, p=mean(fit_data[,i], na.rm = TRUE))
}

fit_data$smoker_na <- ifelse(is.na(nh_diab$age_smoking), 1, 0)
fit_data$smoker_yes <- ifelse(nh_diab$age_smoking > 0 & !is.na(nh_diab$age_smoking), 1, 0)
fit_data$age_smoking <- ifelse(is.na(nh_diab$age_smoking), 0, nh_diab$age_smoking)

race_cols <- dummy_cols(fit_data$race)
colnames(race_cols) <- c("race","race_mex_amer","race_other_hisp","race_white","race_black","race_other")
fit_data <- cbind(fit_data, race_cols[,-1])
fit_data <- fit_data %>% select(-id, -race)
```

# Machine Learning Pipeline
Now that we have a full dataset, we need run an ML pipeline to build a model and evaluate its predective abilities.  For this assignment, I'll be using a Support-Vector Machine model.  The data was split into an 66/33 train/test split.

```{r, include = FALSE}
## split data into a train and test set
index <- 1:nrow(fit_data)
testindex <- sample(index, trunc(length(index)/3))
testset <- fit_data[testindex,]
trainset <- fit_data[-testindex,]
```

To get a sense of the relationship between the covariates that could be in the model, let's check out the correlation matrix of the data:
```{r, echo=FALSE}
data.frame(cor(fit_data))
```
Examining the correlations between variables, we see there is a high correlation between height and upper_leg_length, and weist and weight, thought not much else.  I think it is safe to leave all variables in the model -- even with the high correlation between a couple of the covariates, with our sample size dimensionality issues are not particularly a concern, and there are perhaps non-linear relationships between any of the correlated covariates and the reponse variable that might help improve the model.

First, let's do a grid search across the hyperparameter space, getting the mean of AUCs from models fit using 10-fold cross validation on each parameter setting.  The grid search will be across an exponential grid from 10^-2 to 10^5.
```{r, include = FALSE}
file_bool <- file.exists("../Robjects/cv_list.Rdata")
if(file_bool == TRUE){
  
  load("../Robjects/cv_list.Rdata")
  
} else {
  gamma_range <- 10^(-2:3)
  cost_range <- 10^(-2:3)
  
  flds <- createFolds(trainset$diabetic, k = 10, list = TRUE, returnTrain = FALSE)
  
  cv_auc <- list()
  for(g in 1:length(gamma_range)){
    for(c in 1:length(cost_range)){
      
      gamma <- gamma_range[g]
      cost <- cost_range[c]
      hyper_setting <- paste0("gamma: ", gamma, " cost: ", cost)
      auc_vec <- c()
      
      for(f in 1:10){
      print(paste0("Fold ", f))
      start_time <- Sys.time()
      trainset_fold <- trainset[flds[[f]],]
      testset_fold <- trainset[-flds[[f]],]
      print(hyper_setting)
      print(paste0(nrow(trainset_fold), " ", nrow(testset_fold)))
      svm.fold.model <- svm(diabetic ~ ., data = trainset_fold, cost = cost, gamma = gamma)
      svm.pred <- predict(svm.fold.model, testset_fold[,-15])
      pr <-prediction(svm.pred, testset_fold$diabetic)
      auc <- performance(pr, measure = "auc")@y.values[[1]]
      auc_vec <- c(auc_vec, auc)
      
      print(paste0("AUC: ", auc))
      end_time <- Sys.time()
      print(paste0("Time to fit: ", end_time - start_time, " secs"))
      
      }
      cv_auc[[hyper_setting]] <- mean(auc_vec)
    }
  }
  save(cv_auc, file = "../Robjects/cv_list.Rdata")
}
```


The max AUC from the parameters searched is `r round(max(unlist(cv_auc)), 3)`, which was attained with the hyperparameter setting gamma = 0.01, cost = 0.01.  Now, let's fit the SVM on the entire training set and see how it performs on the test data.
```{r, include = FALSE}
gamma <- 0.01
cost <- 0.01
svm.model <- svm(diabetic ~ ., data = trainset, cost = cost, gamma = gamma)
svm.pred <- predict(svm.model, testset[,-15])
```


```{r, echo = FALSE}
pr <- prediction(svm.pred, testset$diabetic)
prf <- performance(pr, measure="tpr",x.measure="fpr")
plot(prf, main="SVM ROC Curve")
lines(x = c(0,1), y = c(0,1),col="blue")
```
```{r, echo=FALSE}
opt.cut = function(perf, pred){
    cut.ind = mapply(FUN=function(x, y, p){
        d = (x - 0)^2 + (y-1)^2
        ind = which(d == min(d))
        c(sensitivity = y[[ind]], specificity = 1-x[[ind]], 
            cutoff = p[[ind]])
    }, perf@x.values, perf@y.values, pred@cutoffs)
}
print(opt.cut(prf, pr))
print(paste0("AUC    ", performance(pr, measure = "auc")@y.values[[1]]))
```

While using a model like a support vector machine can lead to the superior classification performance compared to more interpretable models, it is known to be a black box algorithm.  A way to evaluate how importance each of the variables is to go through each variable, perturb each observation and get a prediction with that perturbed data included, seeing how much the loss function changes whent this is done.  Average the change in loss for each variable -- this gives an estimate of how important the variable is in the model.  The perturbations were done as follows: perturb to the mean for numeric variables, perturb to median for categorial, flip from 0->1 or 1->0 for binary.  The importance of the variables are listed in decresing order below.
```{r, include = FALSE}
numeric_column_indexes <- c(1,2,3,4,5,6,9,13,14)
ordinal_column_indexes <- c(8,11,16)
binary_column_indexes <- c(17,18,19,20,21,22,23)
```

```{r, include = FALSE}
importance <- c()
for(c in 1:ncol(testset)){
  print(c)
  
  perturb <- trainset
  
  if(c != 15){
  if(c %in% numeric_column_indexes){
    perturb[,c] <- mean(testset[,c])
  } else if (c %in% ordinal_column_indexes){
    perturb[,c] <- median(testset[,c])
  } else if (c %in% binary_column_indexes){
    perturb[,c] <- ifelse(perturb[1,c] == 1, 0, 1)
  } else {
    perturb[,c] <- ifelse(perturb[1,c] == 2, 1, 2)
  }
  
  svm.pred <- predict(svm.model, perturb[,-15])
  svm.class <- ifelse(svm.pred > 1.9673844, 2, 1)
  avg_cost <- sum(abs(svm.class - perturb[,15]))/nrow(perturb)

  importance <- c(importance, avg_cost)
  }
}
```

```{r, echo = FALSE}
names(importance) <- colnames(testset[-15])
print(round(sort(importance, decreasing = TRUE),3))
```
These numbers can be roughly interpreted as the average absolute error of the predictions, considering the given variable had been perturbed. Not a whole lot can be said here -- it can be observed age is the most important variable.  race_other_hisp (The non-Mexican-American hispanic racial category), also has a much higher importance compared to the other covariates.  Race_white and (oddly) smoker_yes (the boolean variable indicating whether somone was a smoker) are of the lowest importance.  It can also be said though, that the similar importance of most of the variables points towards why SVMs are difficult to interpret, and that this is not a very satifsfactory conclusion